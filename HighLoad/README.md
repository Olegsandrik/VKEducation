# HighLoadTechnoPark
Репозиторий работы по высоконагруженным системам Технопарка от ВК

# Тип сервиса
Онлайн кинотеатр, аналогом которого является Кинопоиск 
# MVP
1) Регистрация и авторизация пользователя.
Описание: Сервис будет предоставлять возможность регистрации и авторизации на сайте.
2) Поиск фильма или сериала в каталоге.
Описание: Поиск по описанию или по названию фильма/сериала.
3) Просмотр описания и комментариев видеоматериала.
Описание: Существует возможность до просмотра ознакомиться с контентом, а так же в случае покупки подписки можно прочитать комментарии профессиональных кинокритиков.
4) Просмотр видеоматериала.
Описание: Просмотр в 4k только для зрителей с платной подпиской и 1080p для остальных пользователей, с условием ограниченного доступа к видеоматериалам.
5) Выставление оценки видеоматериалу.
Описание: Для зарегистрированных пользователей существует возможность поставить оценку и написать комментарий после просмотра.
6) Подписка.
Описание: Подписка (1/6/12/24 месяцев) для просмотра в 4к, а также приобретение определенного видеоматериала.
# Целевая аудитория
Пользователями являются граждане России и СНГ возраста от 12 до 65 лет. Около 25 млн и 10 млн постоянных пользователей России и СНГ соответсвенно, а также около 3-4 млн и 2 млн пользователей ежедневно.

### Рисунок 1 (географическое расположение пользователей сервиса [1])
![image](https://github.com/user-attachments/assets/ebf25d48-de9a-46a1-917a-e1b4e0fcae6f)

# Продуктовые метрики

| Месячная аудитория  | Дневная аудитория |
| ------------- | ------------- | 
| 22 млн [2] | 2.5 млн [2] |

## Подсчет размера хранилища:
Из источника номер три следует, что всего 80 тысяч единиц контента. В среднем серия сериала 4k длительностью час весит 20 Гб, а полноценный фильм 4k длительностью 2,5 часа около 50 Гб следовательно 35 Гб в среднем для материала в разрешении 4к, а значит **3 петабайта для 4к** данных весит вся библиотека Кинопоиска. Однако на серверах будут храниться не только копии 4к, но и копии 1080р и 360р. Приблизительные коэффициенты уменьшения размера файла для различных разрешений: 1/4 от размера файла 4K для 1080р и 1/18 от размера файла 4K для 360р, таким образом получаются значения 7 Гб * 80000 = 560000 ГБ = 560 ТБ = **0,56 ПБ
для 1080р** и 1.75 Гб * 80000 = 140000 ГБ = 140 ТБ = **0,14 ПБ для 360р**. Соответственно **размер общего хранилища получается 3,7 Пб**. Некоторые тайтлы, выпущенные за долго до 2020-х и имеющие максимальное разрешение 360р, были искусственно улучшены нейросетями и доведены до качества 1080р или выше, что позволяет при вычислении объема хранилища пренебречь материалами, которые не изначально не имеют качетсво 1080 и выше.
Для того, чтобы хранить персональные данные пользователя нужно понимать, что он может включать текстовую информацию (имя, возраст, биография), а также фотографии. Текстовые данные занимают несколько килобайт, а фотографии могут варьироваться от 100 Кб до нескольких мегабайт. Так с верхней оценкой по этому параметру как 20 Мб, то **20 Мб** на пользователя в среднем.
Всего на храниение данных пользователя требуется выделить около **0,5 Тб**. 
Одна рецензия ограничена 2 тысячами символов, таким образом в ASCII одна рецензия весит не более 2 Кб, следовательно 2*827824 = 1655648 Кб = 1,6 Гб. Для запаса выделим **2 Гб** для того, чтобы там же хранить оценки от 1 до 10.
| Размер хранилища на человека | Размер хранилища на всех пользователей | Размер хранилища на весь видеоконтент | Размер хранилища на рецензии и оценок |
| ------------- | ------------- | ------------- | ------------- |
| 20 Мб | 0.5 Тб | 3,7 Пб | 2 Гб |


## Подсчет среднего количества действий пользователя по типам в день:
Регистрацию нет смысла считать, а авторизацию за день человек делает в среднем **1 раз**, так как в сервисе для просмотра **требуется авторизация как минимум**.
Покупает человек подписку раз в пару месяцев, значит **0.16 раз** в день. 
В 2023-м каждый подписчик в среднем увидел в онлайн-кинотеатре 34,5 тайтлов. Больше всего подписчики смотрели фильмы — в среднем по 22,6 фильма.) Таким образов в месяц это 2.9 тайтла (для простоты это будут фильмы [4] в месяц, значит 1/10 фильма в день или **16-18 минут**.
За 2023 год в среднем в месяц было 180 млн оценок [5], таким образом 6 млн оценок в день или **0.27 оценок** у одного пользователя в день.
Всего рецензий 827824, ежедневно публикуются сотни [6]. Таким образом пусть будет в среднем 400 рецензий в день, следовательно **0.000018** в день на человека.
Поиск контента пользователь использует около **3-4** раз в день.

## Таблица подсчетов
| Действие пользователя | Частота действий в день |
| ------------- | ------------- | 
| Регистрация и авторизация | 1 |
| Покупка подписки | 0.16 |
| Поиск видеоматериала | 4 |
| Оставление комментария | 0.000018 |
| Просмотр видеоматериала | 16-18 минут |
| Оценка видеоматериала | 0.27 |


# Технические метрики

## RPS в разбивке по типам запросов
| Действие  | Формула | RPS |
| ------------- | ------------- | ------------- |
| Регистрация и авторизация  | 1*2500000/86400  | 29  |
| Поиск фильма  | 4*2500000/86400  | 115 |
| Оставление комментария | 0.000018*2500000/86400 | 0.0005 |
| Просмотр видеоматериала  | (17*60)*2500000/84600 | 30141 |
| Платная подписка  | 0.16*2500000/86400 | 4.6 |
| Оценка видеоматериала | 0.27*2500000/86400 | 7.8 |

## Сетевой трафик:
Битрейт видео на КиноПоиске может варьироваться в зависимости от качества контента и конкретного фильма или сериала. Обычно для потокового видео в HD разрешении битрейт составляет от 3 до 5 Мбит/с, а для 4K — от 15 до 25 Мбит/с. Ввиду того, что с платной подпиской нашим сервисом пользуется только 50% пользователей, но при этом 80% из них потребляют контент либо с мобильного телефона, либо с ноутбука с разрешением 1080р, то целесообразно вычислить 10%-ный общий трафик, использующий формат 4к. Для этих **10% для расчета будут взяты 20 Мбит/c**. Остальные же **90% будут использовать 3 Мбит/c** с погрешностью на просмотр 360р при плохом интернет соединении. Дневная аудитория 2.5 млн и каджый проводит около 17 минут за просмотром контента. В результате расчета будет 43 млн минут в день. 
Расчет:
* Формула для 10%: (4.300.000*60)*20 = 5 160 000 000 Мбит за весь день или **0,645 как суммарный сетевой трафик на 4к видеоматериал**.
* Формула для 90%: (38.700.000*60)*3 = 6 966 000 000 Мбит за весь день или **0,871 как суммарный сетевой трафик на 1080р и 360р видеоматериал**.
  
Пиковые нагрузки на сервис ориентировочно будут по вечерам буднечных дней или будут на выходных (возможны стихийные изменения как, например, финал ЛЧ по футболу или релиз новой серии популярного сериала, но под такие случаи выделяется дополнительное железо на некоторое время). Это время, когда большинство людей заканчивают работу и начинают отдыхать. Люди могут иметь привычку использовать интернет в вечернее время для развлечения. В моменты пиковой нагрузки возрастать среднее значение сетевого трафика будет **примерно втрое** из-за вышеперечисленного паттерна. 
Для 10% таким образом получается **60 Гбит/с** как среднее значение (5160000/(24*60*60)=60 Гбит/c), а пиковое значение является средним, умноженным на 3, то есть **180 Гбит/с**.
Для 90% таким образом получается **80 Гбит/с** как среднее значение (6966000/(24*60*60)=80 Гбит/c), а пиковое значение является средним, умноженным на 3, то есть **240 Гбит/с**.
Остальные же три минуты в сервисе выполняются оставшиеся действия из мвп.


## Сетевой трафик 10% - обычный (78 Гбит/с)
| Действие  | Скорость трафика |
| ------------- | ------------- |
| Просмотр видеоматериала  | 60 Гбит/с |
| Регистрация и авторизация  | 3 Гбит/c | 
| Поиск фильма  | 8 Гбит/c | 
| Оставление комментария | 5 Гбит/c |
| Платная подписка  | 1 Гбит/c |
| Оценка видеоматериала | 1 Гбит/c|

## Сетевой трафик 10% - пиковый  (235 Гбит/с)
| Действие  | Скорость трафика |
| ------------- | ------------- |
| Просмотр видеоматериала  | 180 Гбит/с |
| Регистрация и авторизация  | 10 Гбит/c | 
| Поиск фильма  | 24 Гбит/c | 
| Оставление комментария | 15 Гбит/c |
| Платная подписка  | 3 Гбит/c |
| Оценка видеоматериала | 3 Гбит/c |


## Сетевой трафик 90% - обычный (98 Гбит/с)
| Действие  | Скорость трафика |
| ------------- | ------------- |
| Просмотр видеоматериала  | 80 Гбит/с |
| Регистрация и авторизация  | 3 Гбит/c | 
| Поиск фильма  | 8 Гбит/c | 
| Оставление комментария | 5 Гбит/c |
| Платная подписка  | 1 Гбит/c |
| Оценка видеоматериала | 1 Гбит/c|

## Сетевой трафик 90% - пиковый  (298 Гбит/с)
| Действие  | Скорость трафика |
| ------------- | ------------- |
| Просмотр видеоматериала  | 240 Гбит/с |
| Регистрация и авторизация  | 10 Гбит/c | 
| Поиск фильма  | 24 Гбит/c | 
| Оставление комментария | 15 Гбит/c |
| Платная подписка  | 3 Гбит/c |
| Оценка видеоматериала | 3 Гбит/c |

# Глобальная балансировка нагрузки
## Функциональное разбиение по доменам
Для примера доменное имя моего сервиса будет **FilmHub.ru**.
Сервис будет работать на домеенных зонах Казахстана и Беларуси: 
* **FilmHub.kz**
* **FilmHub.by**

Все версии, которые будут расмотренны далее, кроме comments.Filmhub.ru, будут иметь аналоги и в доменных зонах остальных двух стран.
Тогда для мобильных пользователей, которые составляют около 50% от общего числа пользователей, будет **m.FilmHub.ru**, который оптимизирован для работы с мобильными устройствами. Домен **passport.FilmHub.ru** выполняет аутентификацию пользователей. А также **api.Film.ru** для интеграции с сервисами оплаты или рекламы. **comments.Filmhub.ru** для комментариев. 

## Обоснования расположения ДЦ
Дата центры обязательно распологаются в столицах: Москва, Астана, Минск, как наибольшее скокпление ЦА. Ввиду огромной территории РФ и наличия большей части аудитории именно оттуда, то стоит расположить так же ДЦ по всей России в крупных городах, таких как: Санкт-Петербург, Краснодар, Казань, Екатеринбург, Новосибирск, Красноярск, Якутск, Хабаровск и Магадан и Новый Уренгой. Эти города наиболее лучшим образом покрывают всю территорию РФ. Еще добавим Алматы из Казахстана для более качественного покрытия Казахстана. 
### Рисунок 2 (схема распределения ДЦ)

![image](https://github.com/user-attachments/assets/852b3e4d-a198-41fb-8a8b-de87bad419b9)

Ссылка на Яндекс Карту: https://yandex.ru/maps/?um=constructor%3A503b82edec1e14fae8d2530b4f6322524f3c2279ef57a085767b4b01a5968f02&source=constructorLink

## Расчет распределения запросов из секции "Расчет нагрузки" по типам запросов по датацентрам
Каждый ДЦ хранит полный объем видеоматериала, доступного на сервисе, поэтому запрос на просмотр или поиск фильма отправляется на ближайший ДЦ. Однако регистрация и авторизация, а также запрос на оформление платной подписки пользователя доступны только в Москве, Питере, Минске, Астане и Красноярске (рисунок 3). Так что соседние ДЦ проксируют запросы на авторизацию, регистрацию и оформление платной подписки на ближайший ДЦ с менее загруженным сервером авторизации (Например: Краснодар - Минск, Новый Уренгой - Питер, Магадан - Красноярск). Для согласованности данных во всех ДЦ запрос на оставление комментария и на оценку видеоматериала возможен через ДЦ Москвы (рисунок 4). Запросы будут балансироваться на уровне DNS благодаря записям passport.FilmHub.ru, которая выполняет авторизацию и регистрацию, api.Film.ru, которая реализует для интеграции с сервисами оплаты или рекламы, а также 

### Рисунок 3  (красным выделены ДЦ с возможностью запросов на регистрацию, авторизацию и оформление платной подписки)

![image](https://github.com/user-attachments/assets/e1551d8b-e77f-4eab-b1b2-840680e3880d)


### Рисунок 4 (красным выделены ДЦ с возможностью запросов на оставление комментария и оценки)

![image](https://github.com/user-attachments/assets/64a3d868-215e-4285-b0cd-b8b13c5f684e)


## Схема DNS балансировки
**Latency-based DNS (GeoDNS)** позволит выдавать адрес ближайшего ДЦ с минимальным RTT, на что и рассчитано расположение наших ДЦ. 


## Механизм регулировки трафика между ДЦ
Если ближайший с физической точки зрения сервер перегружен, то запрос отправиться по **Based latancy balancing**, например человек из города Чита направляет запрос на ДЦ Красноярска, однако тот перегружен. В этом случае запрос пользователя будет направлен на сервер с минимальной задержкой, например это будет ДЦ Хабаровска. Этот пример иллюстрирует наглядно механизм регулировки трафика между ДЦ в критических случаях, который может продолжаться и более чем в две итерации.


# Локальная балансировка нагрузки

## Схема маршрута при локальной балансировке

![image](https://github.com/user-attachments/assets/6a7e33d4-8459-4420-9538-ebaef67e2d3d)


## L4
Был выбран VS IP tunneling, так как балансировщик инкапсулирует сообщения, после обработки которых не требуется возвращение через балансировщик. Что позволяет убрать лишний трафик в приложении и разгрузить балансировщик для выполнения полезной работы, что в общем повышает производительность системы. CARP (Common Address Redundancy Protocol) объединяет несколько устройств в группу, определяя master устройство, после чего начинают работу. Если master выходит из строя, то один из резервных берёт на себя его функции. Чаще всего в группе будет пара балансировщиков на случай выхода одного из строя. Проверка сервера на доступность будет происходить специальным запросом (health check) определенной кастомной пустой функции. L4 балансировщик выбирает наименее загруженный Nginx и отправляет запрос туда. 

## L7
Выбор пал на Nginx ввиду его универсальности и способности качественной работы с "медленными клиентами". Reverse-proxy выполняет следующие действия:

**Кэширование:** реверс прокси nginx может кэшировать ответы от серверов-источников, чтобы уменьшить количество запросов к серверам-источникам и ускорить время ответа.

**Сжатие:** реверс прокси nginx может сжимать ответы от серверов-источников, чтобы уменьшить объем передаваемых данных.

**Защита:** реверс прокси nginx может выполнять функции защиты, такие как проверка подлинности и авторизация, чтобы ограничить доступ к серверам-источникам. Защита от DDos.

**Мониторинг:** реверс прокси nginx может мониторить состояние серверов-источников и автоматически переключаться на другой сервер, если один из них становится недоступным.

**Работа с https/http:** расшифровывает https запросы и после передает серверам http запросы.

После Nginx проксирует запрос на соответсвующий кластер, определяемый по заголовку.

## Kubernetes
Для того, чтобы разворачивать многочисленные экземпляры программного обеспечения будет использоваться kubernetes. Основными сущностями, с которыми работает kubernetes:
* Pod: Это основная единица в Kubernetes, представляющая собой один или несколько контейнеров, которые совместно используют сетевые и хранилищные ресурсы. Чаще всего Pod содержит один контейнер.
* Node: Физический или виртуальный сервер, на котором запускаются Pods.
* Cluster: Набор узлов (Nodes), объединенных в единое целое для запуска контейнерных приложений. В кластере Kubernetes есть как минимум один мастер-узел и несколько рабочих узлов.

Kubernetes автоматически выполняет рутинные действия, запуская кластеры с определенными конфигурациями. А также имеется auto-scalling, что позволяет полноценно использовать ресурсы hardware. Kubernetes предоставляет возможности балансировки нагрузки, которые распределяют входящий трафик между несколькими репликами контейнера. Kubernetes может автоматически обнаруживать и перезапускать контейнеры, которые упали или становятся недоступными. Очень эффективно и удобно для реализации программного обеспечения сервиса.
Для реализации сервисов выполняющих авторизацию и регистрацию, сервисов выполняющих оформление платной подписки на соответсвующем дата-центре Москвы, Питера, Минска, Астаны и Красноярска будет развернут отдельный кластер с повышенной безопасностью. Кроме того московский ДЦ в отдельном кластере разворачивает сервис, позволяющий комментировать и ставить оценки, для того чтобы улучшить праметры безопасности и изоляции от других сервисов. k8s берет на себя распределение и перераспределение физических ресурсов, обеспечивая непрерывную работу сервисов, посредством их контейнеризации.

# Логическая схема БД

![image](https://github.com/user-attachments/assets/fa822980-2a2d-4186-a67b-a46e08c9d5be)

### Схема связей

* Films (1) <--- (M) Reviews
* Films (1) <--- (M) Film_ Genres (M) ---> (1) Genres
* Films (1) <--- (M) Film_Actors (M) ---> (1) Actors
* Users (1) <--- (M) Reviews
* Users (1) <--- (1) Avatars
* Actors (1) <--- (M) Photos
* User (1) <--- (M) Session

### Расчет размера данных и нагрузки на чтение/запись

| Таблица |	Размеры данных |	Консистентность | Нагрузка на чтение (RPS) | Нагрузка на запись (RPS) |
|--------------| ----------------| --------------|----------------| --------------|
| Films | ~500 байт на 1 фильм * 80 тысяч единиц контента * 4 разрешения = **0.148 ГБ**| при удалении фильма удаляются все film_actors и film_geners, reviews | 30141/18/60=28 | 10 |
| Actors |  ~480 байт * 80 000 человек = **0.0373 GB** | при удалении удаляются все photos и Film_Actors | 30141/18/60=28 | 10 |
| Users | ~500 байт * 22 млн = **10.24 GB** | при удалении удаляются все avatars и rewies, session | 29 | 29 |
| Reviews | ~400 байт * 180 млн оценок = **67.11 GB** | - | 30141/18/60=28 | 0.0005 |
| Genres | ~150 байт * 100 =  **0.000014 GB** | удаляюся все Film_Genres | 30141/18/60=28 | 0 |
| Photos | ~250 байт * 22 000 * 1/3 = **000.1705 GB**| - | 30141/18/60=28 | 3 |
| Avatars | ~250 байт * 100 000 = **0.0233 GB**| - | 29 | 0.5 |
| Film_Actors | ~16 байт * 200 000 записей = **0.00298 GB**  | - | 30141/18/60=28 | 10+10=20 |
| Film_Geners | ~12 байт * 160 тысяч единиц контента * 3 = **0.00534 GB**| - | 30141/18/60=28 | 10 |
| Session |  ~324 байта * 365 * 2 500 000 = **275.6 ГБ** | - | 29 | 145 |

Кинопоиск существует с 7 ноября 2003, то есть 7650 дней. Всего 80 000 тайтлов => добавляется примерно 10 тайтлов в день. Всего актеров около 80 000 => 10 новых актеров в день. Новые жанры не придумываются почти никогда => 0. 22 000 фоток => 3 фотки в день.
Avatars нагрузка на запись примерные значения, исходя из поведения пользователей.
При авторизации пользователь получает jwt-token, срок действия которого ограничен 12-тью часами. Поэтому каждый пользователь в среднем раз в день авторизуется. Если таких пользователей 2.5 млн, то в результате вычисления получим 28.9351851852~29 rps на запись. Чтение jwt-token'а будет происходить каждый раз при потыке просмотра контента, просмотра профиля, оставления комментария и других действий, требующих авторизации. Таким образом таких действий пользователь совершает около 5 за один день.  
# Физическая схема БД
### Выбор СУБД
* Для поиска фильмов по названию и описанию будет использоваться **Elasticsearch**.
* Для хранения таблиц Films, Genres, Actors, Films_Actors, Users будет использоваться **PostgreSQL**.
* Для выдаче данных о фильме или актере на соответствующих страницах будет использоваться **Tarantool**, который с некоторой переодичностью производит обновление кеша, нагружая postgreSQL.
* Для хранения таблиц Reviews использоваться будет Cassandra, так как ввиду особенности записи данных в Cassandra она является наиболее удобной для быстрого чтения комментариев пользователей.
* Для Avatars, Photos и видеоконтента использоваться будет **S3**.
* Для рекомендаций на основе рейтинга и кол-ва просмотров фильма, а также хранения сессионных данных будет использоваться **Redis**.
Redis используется для управления временными данными сессий. 
* Для статистики будет использоваться **ClickHouse**.
* Для мониторинга будет использоваться **Prometheus**.
Prometheus тесно интегрируется с Kubernetes, что делает оптимальным выбором для мониторинга контейнеризованных приложений. Он может автоматически обнаруживать сервисы и поды в кластере K8s.
* Для хранения конфигурации K8s будет использоваться **etcd**. Он может использоваться как центральное хранилище конфигурации для распределённых систем, где важно иметь единое, согласованное представление конфигурации.
* И **kafka** будет использоваться для первоначальной обработки данных.
Для первоначального анализа данных о просмотрах видео в фоновом режиме используется kafka, после чего результаты будут записаны в ClickHouse для долговременного хранения. Аналогично будут обработаны такие данные как начало и окончание просмотра видео, комментарии, приостановка видео, авторизация пользователей и другие действия. Эти события отправлены в Kafka, чтобы их можно было обрабатывать асинхронно, не влияя на производительность основного приложения. Также Kafka может служить связующим звеном между микросервисами внутри k8s, обеспечивая надежную и масштабируемую передачу сообщений. Это позволяет микросервисам взаимодействовать друг с другом асинхронно и без блокировок. Используя шины сообщений в обе стороны.

### Физическая схема СУБД

![image](https://github.com/user-attachments/assets/4adb15ba-b13b-410f-b40a-06c8ebe2d06b)

В поле genres Films записаны id соответсвующих жарнов
```
[
  124,
  233,
  ....
]
```


В поле films Actors записаны id соответсвующих Actor_films
```
[
  124,
  233,
  ....
]
```

В поле actors Films записаны id соответсвующих значений Actors_Films
```
[
  123,
  123,
  ....
]

```

В поле photo_urls Actors записано
```
[
  "url_1",
  "url_2",
  ....
]
```

В поле video_url Films (4k, 720p, 1080p, 360p)
```
[
  {"url": "https://example.com/film1.mp4", "quality": "4k"}
  ....
]
```
Остальные типы данных атрибутов всех сущностей являются примитивными типами данных. Такие как: Int, date, text.

Tarantool будет хранить результаты выполнения join Genrs, Actors_Films, Actors к Films, после чего кешировать все результаты ответов для всех запросов. Получаемые таблицы будут весить менее 10 Гб, что подойдет для храения данных в tarantool. Полная денормализация повысила бы сложность записи новых данных, поэтому было принято решение в postgreSQL хранить данные как на схеме для того, чтобы повысить эффективности записи новых значений в СУБД, а также в последствии передовать результаты сложных запросов в tarantool. Передача данных tarantool обосновано тем, что у нас для обычного юзера в приоритете именно чтение, а не запись. Так как любой человек заходя на страницу фильма получает данные об актерах, их ролях и фотографию, а также заходя на страницу актера получает все его фотографии и фильмы. Такое кеширование данных ощутимо снизит нагрузку на СУБД postgreSQL и позволит повысить эффективность всей системы.

**S3 хранилище**: 

### Схема S3
```
service-content/
  ├── movies/
  │   └── avatar/
  │        ├──4k/
  │        │    └── avatar_4k.mp4
  │        ├──1080p/
  │        │    └── avatar_1080p.mp4
  │        ├──720p/
  │        │    └── avatar_720p.mp4
  │        └──360p/
  │             └── avatar_360p.mp4
  |     
  ├── photos/
  │   ├── high_permission/
  │   │   └── Stethem.png
  │   ├── medium_permission/
  │   │   └── Kolokolnikov.png
  │   └── low_permission/
  │       └── Bortich.png
  └── avatars/
      ├── 2020/
      │   ├── user_id_1/
      │   │        └── user_id_1_avatar.jpeg
      │   ├── user_id_2/
      │   │       └── user_id_2_avatar.jpeg
      │   ├── user_id_3/
      │   │       └── user_id_3_avatar.jpeg
      ├── 2021/
      │   └── ...
      └── 2022/
          └── ...
```        
Бакеты распределены на следующие папки.

movies/ — для хранения фильмов

photos/ — для хранения фотографий (.png для сохранения качества при сжатии)

avatars/ — для хранения аватаров пользователей. (.jpeg для экономии места хранилища)

Для movies будут сделаны подпапки для каждого из разрешений.

Для photos будут сделаны подпапки для высокого, среднего и низкого разрешения. 

Для avatars будут сделаны подпапки для каждого года. 

### Шардирование и резервирование СУБД 

#### Реплицировать будем:

* **Postgres**:
PostgreSQL Streaming Replication: Позволяет настраивать репликацию данных между мастером и репликами. Для многодатацентровой архитектуры можно настроить реплики в каждом датацентре для обеспечения отказоустойчивости и локального чтения.
WAL (Write-Ahead Logging) для передачи изменений на реплику в реальном времени. Мастер - ДЦ Москвы.

* **Elasticsearch**:
Cross-Cluster Replication (CCR): Позволяет реплицировать данные между кластерами Elasticsearch. Реплики во всех ДЦ от ДЦ Мосвкы.

* **Tarantool**:
Replication Protocol: Tarantool поддерживает асинхронную репликацию между экземплярами, что позволяет синхронизировать данные. Создаем мастер и реплику на каждом ДЦ для обеспечения отказоустройчивости.

* **S3 хранилище**:
Будут использоваться мульти-региональные бакеты в AWS S3. Это позволит повысить отказоустойчивость системы и повысить скорость доступа. Cross-Region Replication (CRR) - автоматически реплецируем данные из бакетов Москвы в другие ДЦ. 

#### Шаридировать будем:

* **Redis**:

Используем Redis Cluster и создадим в каждом из ДЦ собственный узел, который будет обслуживать пользователей исходя из региона пользователя. В сервисе авторизации пользователю выдают jwt-token, после чего он сохраняется в сессию на локальом ДЦ.
```
SET session:12345 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCIsImV4cCI6MTYxNjI0MjYyMn0.eyJuYW1lIjoiTmlraXRpbiBBbGV4IiwiaXNfc3ViIjoxNTE2MjM5MDIyLCJ1c2VyX2lkIjo0NDUzNTU2MzU1MjU0NTV9.Mn4iaITlMH8BNZE8tw9TytJQz47z3FjFyux6RWfoL2E' EX 43200

{
  "alg": "HS256",
  "typ": "JWT",
  "exp": 1616242622
}
{
  "name": "Nikitin Alex",
  "is_sub": 1516239022,
  "user_id": 445355635525455
}
HMACSHA256(
  base64UrlEncode(header) + "." +
  base64UrlEncode(payload),
  secret)
```

* **Cassandra**:

Automatic Sharding: Cassandra использует механизм распределения данных по узлам на основе хэширования ключей. Данные автоматически распределяются по кластеру. Ключ партиционирования в Cassandra — это film_id, чтобы централизованно хранить reviews и быстро их получать для фильмов. NetworkTopologyStrategy - для создания реплик на каждом из ДЦ. EACH_QUORUM: Запись подтверждается большинством узлов в каждом ДЦ. Это гарантирует, что данные будут синхронизированы во всех ДЦ при записи нового review. Это увеличит время запроса оставления нового комментария, однако позовлит иметь во всех ДЦ одинаково новые данные.

* **Clickhouse**:

Мы храним данные о лайках, каких-либо комментариев, просмотрах пользователей, статистику пауз и наиболее частых просмотров каких-то сцен фильмов в ДЦ Москвы.
На бекенде данные из Cassandra о комментариях и Click House о лайках комментариев объединяются (сразу всех комментариев одного фильма), сортируются по количеству лайков и отправляются на фронт. Логика, предотварщающая многократное оценивание одним пользователем одного комментари, реализована на беке.

```

likes (
       comment_id UUID,
       user_id UUID,
       timestamp DateTime DEFAULT now()
       PRIMARY KEY (comment_id, user_id)
)

user_views (
       user_id UUID,
       film_id UUID,
       scene_id UUID,
       timestamp DateTime,
       duration UInt32,
       PRIMARY KEY (user_id, timestamp)
)

pause_stats (
       film_id UUID,
       user_id UUID,
       pause_count UInt32,
       total_pause_duration UInt32,
       PRIMARY KEY (film_id, user_id)
) 

popular_scene (
      scene_id UUID
      film_id UUID
      start Time
      end Time
      view_count UInt32,
      PRIMARY KEY (scene_id)
)

```

Distributed Tables, которая маршрутизирует запросы к таблицам по шардам, обращаться к данным в которых можно также и напрямую.
Запрос на запись или чтение в шард может быть отправлен на любую его реплику (реплики для отказоустойчивости). Для данных user_views ключ партиционирования - user_id, для pause_stats - user_id, для popular_scene - film_id. 


### Клиентские библиотеки / интеграции

**Elasticsearch**: Go - olivere/elastic

**PostgreSQL + Citus**: Go - lib/pq (for PostgreSQL), jackc/pgx (for async applications)

**Tarantool**: Go - tarantool/go-tarantool

**Cassandra**: Go - gocql/gocql

**S3-хранилище**: Go - aws/aws-sdk-go

**Redis**: Go - go-redis/redismq

**ClickHouse**: Go - clickhouse/clickhouse-go

**Prometheus**: Go - prometheus/client_golang

**Kafka**: Go - confluentinc/confluent-kafka-go

**etcd**: Go - etcd-io/etcd/client/v3


### Балансировка запросов / мультиплексирование подключений

DataStax Proxy будет мультиплексировать запросы в Cassandra для увеличения эффективности взаимодействия с Cassandra, уменьшая накладные расходы, связанные с несколькими соединениями, которые на большом количестве пользователей будут достаточно ощутимы. 
Odyssey Proxy будет мультиплексировать запросы в PostgreSQL. С помощью Postgres Driver Odyssey может мультиплексировать соединения с Postgres, используя стандартный протокол взаимодействия с базой данных. В случае, если в tarantool закешированных данных нет, пользователь направит запрос в Postgres через Odyssey. Мультиплексирование соединений между Tarantool и Postgres может улучшить производительность системы, уменьшив накладные расходы, связанные с несколькими соединениями. Odyssey увеличит эффективность системы, позволяя ей обрабатывать большее количество одновременных запросов.
DataStax Proxy и Odyssey улучшат масштабируемость всей системы.

### Схема резервного копирования

Поскольку все остальные ДЦ используют S3 хранилище, которое по сути своей являются копией Московского, то есть смысл хранить все backup'ы фильмов главного ДЦ, делая их на магнитной ленте и храня одну копию в самом ДЦ, а вторую в главном офисе компании. Backup'ы будут храниться до 20 лет. Для остальных бизнес-данных сервиса будут делаться икриментальные бекапы (2 полных параллельных бекапа для обеспечения отказоустойчивости в случае повреждения одного из инк. бекапов).

# Алгоритмы

Так как доступ к памяти является узким горлышком с точки зрения быстродействия работы программы, опишем алгоритмы работы с ней. А также формат хранение конфиденциальных данных пользователей и контента.
| Алгоритм |	Область приминения | Мотивационная часть |
|--------------| ----------------| --------------|
| Data Parallelism в S3 | Обработка изображений и видео | Использование с AWS Lambda для обработки данных в S3. Lambda функции мгут быть вызваны параллельно, например, при загрузке новых файлов в S3. |
| AES (Advanced Encryption Standard) | PostgreSQL | Будем шифровать таблицу Users для повышения безопасности всех личных данных пользователя, кроме user_id |
| Серверное шифрование (SSE) | Шифрование данных в S3 | SSE-S3: Использует алгоритм AES-256 для шифрования данных. Ключи шифрования управляются AWS. |
| Рекомендательные алгоритмы фильмов | Выдача рекомендаций пользователю | User-based рекомендации, созданные на основе предпочтений пользователей с похожими интересами (жанрами). Если большое количество пользователей оценивают фильм выше 8, то людям со схожими любимыми жанрами рекомендуется этот видео материал |
| Адаптивная визуализация | Экран пользователя | Специальный алгоритм анализирует размер окна, в котором воспроизводится материал, и адаптивно огранивает под размер окна максимальное качество . Например для экрана 2400 x 1080 не будет возможности трансляции 4к |


# Технологии

| Технологии |	Область приминения | Мотивационная часть |
|--------------| ----------------| --------------|
| Kafka | Обработка потоковых данных, реализация систем обмена сообщениями. | Используется для обработки данных, отправляемых в Click House. |
| NginX | Веб-сервер, обратный прокси, балансировка нагрузки, кэширование. | Используется в системе балансировки и роутинга запросов. |
| ElasticSearch | Поиск и анализ больших объемов данных, логирование, мониторинг, аналитика. | Используется для поиска актеров, фильмов по названию, описанию и жанру. |
| k8s | Управление контейнерами, оркестрация микросервисов, автоматизация развертывания и масштабирования. | В нем будут запущены все backend сервисы. |
| Tarantool | Кэширование | Кэширует запросы в Postgres для уменьшение нагрузки на Postgres. |
| PostgreSQL | Реляционная база данных, поддержка сложных запросов, хранение структурированных данных. | Хранит данные о фильмах, жанрах, пользователях и актерах. |
| S3 | Объектное хранилище для хранения и управления большими объемами данных. | Хранение всех фильмов, фоток и аватарок всего проекта. |
| ClickHouse | Аналитическая база данных, обработка больших объемов данных в реальном времени, BI-решения. | Хранит лайки, просмотры и некоторые продуктовые метрики для последующего анализа и проектирования статистических гипотез. |
| Prometheus | Мониторинг и алертинг для контейнеризованных приложений, сбор метрик. | Используется для хранения и поддержания работоспособности проекта, превентивного устранения ошибок системы. |
| etcd | Хранилище конфигурации и сервисов, управление состоянием распределенных систем. | Для оперативного восстановление сервисов в случае их запуска/перезапуска. |
| Redis | Управление сессиями. | Хранение данных о сессиях пользователя. |
| Cassandra | Работа с большими объемами неструктурированных данных. | Хранит все комментарии пользователей. |
| Odyssey | Прокси-сервер для PostgreSQL, управление подключениями, балансировка нагрузки, кеширование. | Балансирует нагрузку на Postgres |
| DataStax Proxy | Прокси-сервер для Apache Cassandra, управление подключениями, балансировка нагрузки, кеширование. | Балансирует нагрузку на Cassandra |

# Обеспечение надежности

|  | **Что используется**|
|-----------------------------------|-----------------------------------|
| **Базы данных**                   | Используются шардирование, репликация и резервные копии. |
| **Виртуальные компоненты**        | - **Kubernetes**: Автоматически восстанавливает работу неработающих подов или кластера, запуская новый экземпляр. <br>- **Nginx**: Разворачивается в виде нескольких экземпляров, распределяющих нагрузку при выходе из строя одного из них. <br>- **etcd**: Хранит конфигурацию и состояние кластера Kubernetes и Nginx, позволяя быстро восстанавливать работу после перезапуска. |
| **Физические компоненты**         | - **Сервера**: Реализованы группы по логике и кластеризация в Kubernetes для обеспечения работы сервиса при выходе из строя одной из машин. <br>- **Диски**: Используется RAID 5 для распределения данных и информации о четности, позволяющий восстановление данных при сбое. |
| **Мониторинг метрик**             | Сбор метрик с помощью Kafka, привязанного к Prometheus. Позволяет производить мониторинг физического состояния системы, оповещая о проблемах и храня данные о времени отклика, CPU, свободной памяти и скорости чтения/записи дисков.          |
|   **Паттерны**        | Graceful degradation. Реализуется через API-GATEWAY: в случае отказа сервиса возвращает сообщение о "временной недоступности". Например, если сервис комментариев "упал", фронтэнд отображает сообщение о недоступности комментариев, повышая отказоустойчивость системы.   |

# Схема проекта

### Сервисы проекта:
* Сервис авторизации/регистрации: отвечает за регистрацию новых пользователей и авторизацию активных пользователей.
* Сервис оплаты подписки: обрабатывает платежи, управляет подписками и проверяет статус подписки пользователя.
* Сервис комментариев: позволяет пользователям оставлять комментарии к фильмам.
* Сервис просмотра фильмов: предоставляет доступ к видеоматериалам.
* Сервис отображения изображений: отвечает за хранение и предоставление изображений (постеры фильмов, аватарки пользователей, фотографий актеров). 
* Сервис сбора данных: использует ClickHouse для хранения и обработки больших объемов данных о пользователях, фильмах, просмотрах и лайках. Генерирует отчетность для продукта.
* Сервис сбора метрик: использует Prometheus для мониторинга производительности и состояния всех сервисов. Сбор метрик отслеживает использование ресурсов и время отклика.
* Сервис рекомендаций: на основе данных из ClickHouse и общей статистики внутри продукта, сервис будет анализировать статистику и генерировать персонализированные рекомендации фильмов.
* Сервис поиска: позволяет выполнять поиск контента по названию фильма, описанию и актерам.
* Сервис кеширования: кеширует данные в Tarantool из разных СУБД.
* Сервис добавления контента: позволяет добавлять данные на платформу.
* Сервис обновления рейтинга фильма: с определенной переодичностью меняет рейтинг фильма в зависимости от оценок в review.


### Схема взаимодействия сервисов

![image](https://github.com/user-attachments/assets/75e2cb52-d5bf-4dcc-ba99-4dc60b62dcc8)

### Балансировка данных
API Gateway развернут как отдельный сервис в Kubernetes, который будет принимать входящие запросы от клиентов. В каждом ДЦ их несколько. После того, как пользователь отправляет запрос он приходит глобальную балансировку, после чего на выбранный ДЦ запрос проходит локальную балансировку и на отправляется на API-GATEWAY, который в свою очередь собирает нужные данные из всех микросервисов и формирует ответ для пользователя, отдавая его после в Nginx. Поскольку сервис оплаты подписки, авторизации/регистрации и оставления комментариев развернут не на каждом ДЦ, API-GATEWAY отправляет заспрос на соответствующий ДЦ.

### Потоки данных:
* Сервис авторизации: Идет в PostgreSQL за данными о пользователе, сохраняет данные о сессии в Redis.
* Сервис оплаты подписки: В случае оплаты пользовтелем подписки меняет данные в Postgres, а также идет в сервис авторизации для выдачи новой сессии, чтобы jwt содержал свежие данные.
* Сервис комментариев: При запросе комментариев для определенного фильма: получает все комментарии из Cassandra, после чего получает закешированные данные о лайках комментариев, после формирует список комментариев со всеми лайками. При запросе на оставление комментария: записывает новый комментарий в Cassandra.
* Сервис сбора метрик: все сервисы по Kafka отсылают метрики в этот сервис, после чего записываются в Prometheus.
* Сервис рекомендаций: Собирает данные о фильмах из PostgreSQL, о комментариях из Cassandra, о статистике просмотров из Click House, после чего на основе нейронной сети выдает список из 5 фильмов, которые можно порекомендовать пользователю.
* Сервис сбора данных: Kafka асинхронно отправляет данные в сервис, после чего они асинхронно отправляются в Click House.
* Сервис поиска: Черпает данные из СУБД Elasticsearch для поиска. Также этот сервис производит обновление данных в Elastic, черпая их из Postgres.
* Сервис кеширования: Зарпашивает данные из Postgres и Click House, передавая после их в Tarantool.
* Сервис обновления статистики фильмов: на основе комментариев фильмов обновляет рейтинг в Postgres с некоторой переодичностью.
* Сервис просмотра фильмов: на основе подписки пользователя выдает контент фильмов из хранилища S3
* Сервис визуализации изображений: выдает фотки актеров и аватарки из S3 пользователю
* Сервис добавления контента на сайт: Доступен только администратору, позволяет загружать новые фильмы и данные об этих фильмах на площадку. А также фотографии актеров, привязывая их к актерам в Postgres.

# Список серверов

### Базовый расчёт аппаратных ресурсов

| Сервис	| Целевая пиковая нагрузка приложения	| CPU |	RAM	| Net |
|---------|-------------------------------------|-----|-----|-----|
| Сервис авторизации/регистрации (Тяжелая бизнес-логика) | 4*29 RPS | 116 | 23400 MB | 0.1392 Gbit |
| Сервис оплаты подписки (Тяжелая бизнес-логика) | 4*4.6 RPS | 20	| 3680 MB | 0.4416 Gbit/s |
| Сервис комментариев (Средняя бизнес-логика) | 2*0.0005 + 2*30 RPS | 1	| 60 MB | 0.36 Gbit/s |
| Сервис просмотра фильмов (Средняя бизнес-логика) |  2*30141 RPS | 603 | 60282 MB | 2893 Gbit/s |
| Сервис отображения изображений (Легкое JSON API) | 2*430 RPS | 2	| 100 MB | 8.6 Gbit/s |
| Сервис сбора данных (Средняя бизнес-логика) | 2*410 RPS |	8 | 820 MB | 13.12 Gbit/s |
| Сервис сбора метрик (Легкое JSON API) | 2*110 RPS | 1	| 20 MB | 0.528 Gbit/s |
| Сервис рекомендаций (Средняя бизнес-логика) | 2*144 RPS | 3 | 300 MB | 3.456 Gbit/s |
| Сервис поиска (Средняя бизнес-логика) | 2*115 RPS | 3	| 230 MB | 2.76 Gbit/s |
| Сервис кеширования (Легкое JSON API) | 2*62 RPS | 1 | 50 MB | 0.744 Gbit/s |
| Сервис добавления контента (Легкое JSON API) | 2*0.00012040649 RPS | 1 | 1 MB | 0.09259259259 + 0,00000192 Gbit/s |
| Сервис обновления рейтинга фильма (Средняя бизнес-логика) | 2*11 RPS | 1 | 22 MB | 0.132 Gbit/s |



### Рассчет: 
**Все сервисы написаны на Atreugo**

* RPS

Для сервиса сбора данных: Тк собираются данные о лайках, просмотрах, остановках видео и просмотре определенных сцен, то RPS будет достаточно высокий. Лайки имеют 0.27 RPS на фильмы + человек в среднем ставит около 2 лайков в день на комментарий => 2 500 000 * 2 / (60 * 60 * 24) = 60 RPS на новые лайки для комментариев, пользователь в среднем 4-5 раз ставит на паузу за 18 минут просмотра контента => 2 500 000 * 4.5 = 11250000, 11250000/(60 * 60 * 24) = 130 RPS, пользователь смотрит в среднем 1-2 разных материала в день => 1.5 * 2 500 000 = 3750000, 3750000/(60 * 60 * 24) = 43 RPS, в среднем за 18 минут происходит около 6 разных сцен => 2 500 000 * 6 = 15000000, 15000000 / (60 * 60 * 24) = 175 RPS. В сумме получаем около **410 RPS**.

Для сервиса кеширования: Полное обновление данных происходит раз в 30 минут, 80 000 фильмов (новых 10 в день) + 80 000 актеров (новых 10 в день) + за 30 минут 60 * 60 *30 = 108000 лайков => Перезаписываются только измененные данные. Следовательно **62 RPS**.

Для сервиса добавления контента: 7 689 дней (на 25 ноября 2024 года) существует кинопоиск и имеет 80 000  единиц контента => 10 единиц в день или **0.00012040649 RPS**. 

Для сервиса обновления рейтинга фильма: Полное обновление данных происходит раз в 30 минут, всего 1/4 примерно нуждается в обновлении, т. е. 20 000 обновляемых единиц => **11 RPS**. 

Для сервиса сбора метрик: Каждые 30 секунд 1 сервис отсылает данные использования ресурсов и времени отклика. Таким образом получается 5 запросов от 1 сервиса за 30 секунд. 11 сервисов => **110 RPS**.

Для сервиса отображения изображений: Этот сервис грузит аватарки пользователей и фотографии актеров. 1 человек заходя на страницу фильма получает около 5 фотографий актеров и 10 аватарок первых 10 комментариев => 15 единиц контента. Таким образом 2 500 000 * 15 / (60 * 60 * 24)  = **430 RPS**

Для сервиса рекомендаций: Собирает все данные о пользователе и формирует рекомендацию. Для всех 2.5 млн человек в день создавая рекомендацию. На формирование рекомендации одного пользователя требуется запрос в Cassandra, Click и Postgres. То есть около 5 запросов для одного пользователя. 2 500 000 * 5 /(60 * 60 * 24) = **144 RPS**

Для комментариев: Запрос на выдачу комментариев фильма: 2.5 млн человек по 1 фильму = **30 RPS**


* RAM

Atreugo - веб-фреймворк языка Golang. Исходя из расчетов источников [7] и [8] получаем грубые оценки: 

Nginx	SSL handshake (CPS) имеет	(RPS:500, RAM: 10 Mb)

Atreugo тяжелая бизнес-логика (RPS: 1, RAM: 200 Mb)

Atreugo средняя бизнес-логика (RPS: 100, RAM: 100 Mb)

Atreugo легкое JSON API (RPS: 500, RAM: 50 Mb)

* CPU

Тяжелая бизнес-логика: 1 CPU может обрабатывать 1 RPS.

Средняя бизнес-логика: 1 CPU может обрабатывать 100 RPS.

Легкое JSON API: 1 CPU может обрабатывать 500 RPS.

* Net

Net = (RPS * Средний размер запроса + RPS * Средний размер ответа) / 1 Gbit/s 
Так для 10 фильмов с 4 качествами будет добавлено 1000 ГБ контента или 8000 Гбит => 8000 / (24 * 60 * 60) = 0.09259259259 Gbit/s + ответы и запросы

### Выбор модели развертывания

Для оркестрации будет использоваться k8s. 

| Сервис | Количество контейнеров |
|--------|------------------------|
| API Gateway | 14 |
| Nginx | (14+14)*2 + 14|
| Сервис авторизации/регистрации | 8 |
| Сервис оплаты подписки | 8 | 
| Сервис комментариев| 2 |
| Сервис просмотра фильмов | 28 |
| Сервис отображения изображений | 14 |
| Сервис сбора данных | 14 |
| Сервис сбора метрик | 2 |
| Сервис рекомендаций | 2 |
| Сервис поиска | 14 |
| Сервис кеширования | 2 |
| Сервис добавления контента | 2 |
| Сервис обновления рейтинга фильма | 2 |


### Выбор модели хостинга

Для хостинга выбрана модель собственного железа, так как проект крупный и имеет хорошо прогнозируемый профиль нагрузки. 

### Конфигурации

| Сервис	| CPU/req	| CPU/lim |	RAM/req	| RAM/lim | Count | 
|---------|-------|-------|-------|-------|-----|
| Сервис авторизации/регистрации | 8 | 11 | 1600 MB | 3300 MB | 8 |
| Сервис оплаты подписки | 10 | 15 | 1920 MB | 2200 MB | 8 | 
| Сервис комментариев | 1 | 2 | 60 MB | 120 MB | 1 |
| Сервис просмотра фильмов | 43 | 45 | 4305 MB	| 4600 MB | 28 |
| Сервис отображения изображений | 1 | 1 | 100 MB	 | 100 MB	 | 14 |
| Сервис сбора данных | 1 | 1 | 100 MB | 100 MB	| 14 |
| Сервис сбора метрик | 1 | 2 | 20 MB | 40 MB| 2 |
| Сервис рекомендаций | 3 | 3 | 300 MB | 300 MB | 2 |
| Сервис поиска | 1 | 1 | 100 MB | 100 MB | 14 |
| Сервис кеширования | 1 | 1 | 50 MB | 50 MB | 2 |
| Сервис добавления контента | 1 | 1 | 	1 MB | 1 MB | 2 |
| Сервис обновления рейтинга фильма | 1 | 1 | 22 MB | 22 MB | 2 |


# Источники 

[1] https://web.archive.org/web/20181201005421/https://www.alexa.com/siteinfo/kinopoisk.ru 

[2]  https://web.archive.org/web/20231129184042/https://radar.yandex.ru/yandex?month=2022-05

[3] https://www.rbc.ru/technology_and_media/16/11/2023/65548b529a7947815e93d67b

[4] https://www.buro247.ru/news/culture/14-dec-2023-kinopoisk-results-of-the-year.html

[5] https://www.kinopoisk.ru/votes/

[6] https://www.kinopoisk.ru/reviews/

[7] https://blog.nginx.org/blog/testing-the-performance-of-nginx-and-nginx-plus-web-servers

[8] https://www.techempower.com/benchmarks/#section=data-r22&test=fortune&hw=ph
